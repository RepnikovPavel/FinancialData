{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.relativedelta import *\n",
    "from dateutil.easter import *\n",
    "from dateutil.rrule import *\n",
    "from dateutil.parser import *\n",
    "from datetime import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from typing import Dict,Tuple,List\n",
    "import conf\n",
    "from UI.LOG import *\n",
    "import cv2\n",
    "from aml.train_pipeline import *  \n",
    "from aml.train_pipeline import *\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor,FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchinfo import summary\n",
    "\n",
    "# from torchvision.models.detection import ssd300_vgg16,SSD300_VGG16_Weights\n",
    "from torchvision.models.detection.ssd import SSDClassificationHead,SSD300_VGG16_Weights,det_utils\n",
    "from torchvision.models.detection import ssd300_vgg16\n",
    "\n",
    "import torch\n",
    "\n",
    "import aml.model_using as model_using\n",
    "import aml.support_func as support_funcS\n",
    "import aml.time_mesuarment as time_mesuarment\n",
    "import sys\n",
    "\n",
    "import aml.managers as managers\n",
    "import aml.img_processing as img_processing\n",
    "import random\n",
    "import numpy as np\n",
    "import pprint\n",
    "from torchinfo import summary\n",
    "from aml.img_processing import *\n",
    "\n",
    "from PIL import Image\n",
    "import aml.models as models\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint as Print\n",
    "from PIL import Image\n",
    "import warnings\n",
    "from torchvision.utils import draw_bounding_boxes  \n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torchvision.ops import nms \n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision as mAP\n",
    "from matplotlib.transforms import Affine2D\n",
    "import mpl_toolkits.axisartist.floating_axes as floating_axes\n",
    "from IPython.display import IFrame, display, HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "import catboost\n",
    "from io import StringIO \n",
    "import sys\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from catboost import utils\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from tqdm import tqdm\n",
    "import multiprocessing as mp\n",
    "import lightgbm as lgb\n",
    "from numba import jit\n",
    "from aml.pandas_utils import *\n",
    "tqdm.pandas()\n",
    "\n",
    "def simple_plot(x,y,title=''):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(16,9)\n",
    "    ax.plot(x,y)\n",
    "    ax.set_title(title)\n",
    "    ax.grid()\n",
    "\n",
    "    return fig,ax\n",
    "\n",
    "\n",
    "def simple_plot_y1y2(x,y1,y2,label1='',label2='',title=''):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(16,9)\n",
    "    ax.plot(x,y1,label=label1)\n",
    "    ax.plot(x,y2,label=label2)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(which='both')\n",
    "    ax.legend()\n",
    "    return fig,ax\n",
    "\n",
    "def simple_plot_y1y2y3y4(x,y1,y2,y3,y4,label1='',label2='',label3='',label4='',title=''):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(16,9)\n",
    "    ax.plot(x,y1,label=label1)\n",
    "    ax.plot(x,y2,label=label2)\n",
    "    ax.plot(x,y3,label=label3)\n",
    "    ax.plot(x,y4,label=label4)\n",
    "    ax.set_title(title)\n",
    "    ax.grid(which='both')\n",
    "\n",
    "    ax.legend()\n",
    "    return fig,ax\n",
    "\n",
    "\n",
    "def Gini(fpr,tpr):\n",
    "    return 2*metrics.auc(fpr,tpr)-1\n",
    "def plot_gxb_train_results(results):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(16,9)\n",
    "    ax.plot(results['train-auc-mean'],label= 'train-auc-mean')\n",
    "    ax.plot(results['test-auc-mean'],label= 'test-auc-mean')\n",
    "    ax.legend()\n",
    "    return fig,ax\n",
    "\n",
    "\n",
    "def xgb_fit(X,Y, meta_of_fit, path_to_save_model,params,verbose=False):\n",
    "    timer = time_mesuarment.Timer()\n",
    "    timer.start()\n",
    "    eval_metric = params['eval_metric']\n",
    "    model = XGBClassifier(**params)\n",
    "    # print('start model fit')\n",
    "\n",
    "    # X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, train_size=0.8, random_state=1234)\n",
    "    # model.fit(X_train,Y_train, eval_set=[(X_train,Y_train),(X_validation,Y_validation)],verbose=False)\n",
    "\n",
    "    model.fit(X,Y, eval_set=[(X,Y)],verbose=verbose)\n",
    "\n",
    "    # print('train end')\n",
    "    # feature_important = model.get_booster().get_score(importance_type='weight')\n",
    "    # keys = list(feature_important.keys())\n",
    "    # values = list(feature_important.values())\n",
    "    # argsort_ = np.argsort(values)\n",
    "    # print('feauture importance')\n",
    "    # for ind in argsort_:\n",
    "    #     print('{} {}'.format(keys[ind],values[ind]))\n",
    "    results = model.evals_result()\n",
    "    train_loss = np.array(results['validation_0'][eval_metric])\n",
    "    # val_loss = np.array(results['validation_1'][eval_metric])\n",
    "    timer.stop()\n",
    "    print('\"{}\" last train {} {} time_of_train {} min'.format(\n",
    "        meta_of_fit,eval_metric,train_loss[-1],str(timer.get_execution_time()/60)))\n",
    "    model.save_model(fname=path_to_save_model)\n",
    "    \n",
    "\n",
    "\n",
    "def train_model(X,Y, meta_of_fit, path_to_save_model, params,verbose):\n",
    "    # wrap train to another process\n",
    "    try:\n",
    "        target_ = mp.Process(target=xgb_fit,args=(X,Y, meta_of_fit, path_to_save_model, params, verbose))\n",
    "        target_.start()\n",
    "        target_.join()\n",
    "    except:\n",
    "        print('somth went wrong')\n",
    "        raise SystemExit\n",
    "\n",
    "@jit(nopython = True)\n",
    "def get_index_of_time_interval(t, time_intervals):\n",
    "    for i in range(len(time_intervals)):\n",
    "        t1t2 = time_intervals[i]\n",
    "        if t >= t1t2[0] and t < t1t2[1]:\n",
    "            return i\n",
    "        if i == len(time_intervals)-1:\n",
    "            if t >= t1t2[0] and t <= t1t2[1]:\n",
    "                return i\n",
    "\n",
    "@jit(nopython=True)\n",
    "def set_values_to_buffer_by_indexes(buffer,indexes,values):\n",
    "    for i in range(len(indexes)):\n",
    "        buffer[indexes[i]] = values[i]\n",
    "\n",
    "class TimeModel:\n",
    "    models: List[XGBClassifier]\n",
    "    time_intervals: np.array\n",
    "    def __init__(self,path_pattern, time_intervals):\n",
    "        self.models = []\n",
    "        self.time_intervals = time_intervals\n",
    "        for i in range(len(time_intervals)):\n",
    "            model_i = XGBClassifier()\n",
    "            model_i.load_model(fname=path_pattern+str(i)+'.json')\n",
    "            self.models.append(model_i)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        groups_by_time = group_by_time_interval(X,'issue_d',self.time_intervals)\n",
    "        labels = np.zeros(shape=(X.shape[0],), dtype=np.intc)\n",
    "        probas = np.zeros(shape=(X.shape[0],2), dtype=np.float64)\n",
    "        i_ = 0\n",
    "        for time_interval in self.time_intervals:\n",
    "            print(time_interval)\n",
    "            X_ = groups_by_time[i_]\n",
    "            indexes_ = X_.index.values\n",
    "            predict_ = self.models[i_].predict(X_)\n",
    "            probas_ = self.models[i_].predict_proba(X_)\n",
    "            set_values_to_buffer_by_indexes(labels,indexes_,predict_)\n",
    "            set_values_to_buffer_by_indexes(probas,indexes_,probas_)\n",
    "            i_+=1\n",
    "        return labels, probas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(conf.X_train_dataset,index_col=False)\n",
    "Y = pd.read_csv(conf.Y_train_dataset,index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3\n"
     ]
    }
   ],
   "source": [
    "t_1,t_2 = get_min_max(X,'issue_d')\n",
    "delta_t =  3.0\n",
    "N = int((t_2-t_1)/delta_t)+1\n",
    "time_bins = np.linspace(start=t_1,stop= t_2,num=N)\n",
    "time_intervals = [(time_bins[i],time_bins[i+1]) for i in range(len(time_bins)-1)]\n",
    "# plot_float_distribution(X_train['issue_d'],(16,9),'issue date density')\n",
    "groups_by_time = group_by_time_interval(X,'issue_d',time_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2007.495867768595, 2010.8078512396694), (2010.8078512396694, 2014.1198347107438), (2014.1198347107438, 2017.4318181818185), (2017.4318181818185, 2020.743801652893)]\n"
     ]
    }
   ],
   "source": [
    "print(time_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N 9673\n",
      "\"(2007.495867768595, 2010.8078512396694)\" last train auc 0.9999702299136234 time_of_train 0.4863743225733439 min\n",
      "N 153571\n",
      "\"(2010.8078512396694, 2014.1198347107438)\" last train auc 0.8844455011462522 time_of_train 3.103721233208974 min\n",
      "N 773797\n",
      "\"(2014.1198347107438, 2017.4318181818185)\" last train auc 0.7826874402433004 time_of_train 26.615129880110423 min\n",
      "N 262820\n",
      "\"(2017.4318181818185, 2020.743801652893)\" last train auc 0.8442578570939 time_of_train 13.84238266547521 min\n"
     ]
    }
   ],
   "source": [
    "# N 2658\n",
    "# \"(2007.495867768595, 2008.5149396058487)\" last train auc  0.8891242996386302 time_of_train 0.01614310344060262 min\n",
    "# N 5750\n",
    "# \"(2008.5149396058487, 2009.5340114431024)\" last train auc 0.9454247130436597 time_of_train 0.04041525920232137 min\n",
    "# N 18640\n",
    "# \"(2009.5340114431024, 2010.553083280356)\" last train auc  0.9264374299164312 time_of_train 0.11836928129196167 min\n",
    "# N 38865\n",
    "# \"(2010.553083280356, 2011.5721551176098)\" last train auc  0.8507712342316202 time_of_train 0.11460882822672526 min\n",
    "# N 85573\n",
    "# \"(2011.5721551176098, 2012.5912269548635)\" last train auc 0.7977982670191832 time_of_train 0.24786526759465535 min\n",
    "# N 231228\n",
    "# \"(2012.5912269548635, 2013.6102987921172)\" last train auc 0.7610506682236307 time_of_train 0.624234139919281 min\n",
    "# N 488014\n",
    "# \"(2013.6102987921172, 2014.6293706293707)\" last train auc 0.7587616124137769 time_of_train 4.1776900370915735 min\n",
    "# N 785743\n",
    "# \"(2014.6293706293707, 2015.6484424666244)\" last train auc 0.7399517215244317 time_of_train 5.5209632833798725 min\n",
    "# N 1108488\n",
    "# \"(2015.6484424666244, 2016.667514303878)\" last train auc  0.7593030440503921 time_of_train 11.677052640914917 min\n",
    "# N 790043\n",
    "# \"(2016.667514303878, 2017.6865861411318)\" last train auc  0.7522980749334666 time_of_train 6.250174677371978 min\n",
    "# N 494389\n",
    "# \"(2017.6865861411318, 2018.7056579783855)\" last train auc 0.7596001844717735 time_of_train 3.1340223789215087 min\n",
    "# N 241778\n",
    "# \"(2018.7056579783855, 2019.7247298156392)\" last train auc 0.7939667284221041 time_of_train 1.3382912317911784 min\n",
    "# N 35946\n",
    "# \"(2019.7247298156392, 2020.743801652893)\" last train auc  0.9027145554153396 time_of_train 0.11624847253163656 min\n",
    "# n_est_by_time_interval_index = [\n",
    "    \n",
    "# ]\n",
    "# max_depth_vec = [\n",
    "#     5,  5,   5,  5, 6 , 7,  7, 7, 7, 7, 7, 6, 5\n",
    "# ]\n",
    "# if len(n_est_by_time_interval_index) != len(time_intervals):\n",
    "#     raise SystemExit\n",
    "# if len(max_depth_vec) != len(time_intervals):\n",
    "#     raise SystemExit\n",
    "\n",
    "for i in range(len(time_intervals)):\n",
    "    ith_time_interval = time_intervals[i]\n",
    "    X_ = groups_by_time[i]\n",
    "    Y_ = Y.loc[X_.index]\n",
    "    # n_estimators = n_est_by_time_interval_index[i]\n",
    "    # max_depth = max_depth_vec[i]\n",
    "    N = len(Y_)\n",
    "    # min_child_weight = 0\n",
    "    eval_metric = 'auc'\n",
    "    # if N < 10:\n",
    "    #     min_child_weight = 1\n",
    "    # if N>= 10**1 and N < 10**2:\n",
    "    #     min_child_weight = 3\n",
    "    # if N>= 10**2 and N < 10**3:\n",
    "    #     min_child_weight = 5\n",
    "    # if N>= 10**3 and N < 10**4:\n",
    "    #     min_child_weight = 10\n",
    "    # if N >= 10**4 and N < 10**5:\n",
    "    #     min_child_weight = 20\n",
    "    # if N >= 10**5:\n",
    "    #     min_child_weight = 30\n",
    "    print('N {}'.format(N))\n",
    "\n",
    "    params = {\n",
    "            'eta': 0.01,\n",
    "            'max_depth': 8,\n",
    "            'n_estimators': 4000,\n",
    "            'gamma':  0.2,\n",
    "            'colsample_bytree':0.5, \n",
    "            'colsample_bylevel':0.5, \n",
    "            'colsample_bynode':0.5,\n",
    "            'lambda': 1.5,\n",
    "            'alpha': 0.1,\n",
    "            'scale_pos_weight': np.sum(np.where(Y==0))/np.sum(np.where(Y==1)),\n",
    "            'min_child_weight': 5,\n",
    "            'max_delta_step': 5,\n",
    "            'objective':'binary:logistic',\n",
    "            'eval_metric': eval_metric, \n",
    "            'tree_method':'gpu_hist',\n",
    "            'n_jobs':8,\n",
    "            'gpu_id':0,\n",
    "            'seed':0,\n",
    "            'subsample': 0.9,\n",
    "    }\n",
    "    train_model(X=X_,Y=Y_,meta_of_fit=str(ith_time_interval),path_to_save_model=conf.xgb_models_pattern+str(i)+'.json',params=params,verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3\n",
      "(2007.495867768595, 2010.8078512396694)\n",
      "(2010.8078512396694, 2014.1198347107438)\n",
      "(2014.1198347107438, 2017.4318181818185)\n",
      "(2017.4318181818185, 2020.743801652893)\n"
     ]
    }
   ],
   "source": [
    "model = TimeModel(path_pattern=conf.xgb_models_pattern,time_intervals= time_intervals)\n",
    "eval_preds_,eval_proba_ = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def apply_limits(probas):\n",
    "    o_ = np.zeros(shape=(len(probas),))\n",
    "    for i in range(len(probas)):\n",
    "        if probas[i]>0.999:\n",
    "            o_[i]= 0.999\n",
    "            continue\n",
    "        if probas[i] < 0.001:\n",
    "            o_[i] = 0.001\n",
    "            continue\n",
    "        o_[i] = probas[i]\n",
    "    return o_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC 0.8130930423869495\n",
      "Gini 0.6261860847738989\n"
     ]
    }
   ],
   "source": [
    "fpr,tpr,tr = metrics.roc_curve(Y,eval_proba_[:,1],pos_label=1)\n",
    "# simple_plot(fpr,tpr,title='ROC XGBoost')\n",
    "print('AUC {}'.format(metrics.auc(fpr,tpr)))\n",
    "print('Gini {}'.format(Gini(fpr,tpr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3\n",
      "(2007.495867768595, 2010.8078512396694)\n",
      "(2010.8078512396694, 2014.1198347107438)\n",
      "(2014.1198347107438, 2017.4318181818185)\n",
      "(2017.4318181818185, 2020.743801652893)\n"
     ]
    }
   ],
   "source": [
    "X_eval = pd.read_csv(conf.X_test_dataset,index_col=False)\n",
    "submit_preds_,submit_proba_ = model.predict(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: answer.csv (deflated 57%)\n"
     ]
    }
   ],
   "source": [
    "def make_submission(positive_probabilities):\n",
    "    d = {'index': [i for i in range(0,len(positive_probabilities))], 'loan_status': positive_probabilities}\n",
    "    o_df = pd.DataFrame(data=d)\n",
    "    submission_csv = os.path.join('answer.csv')\n",
    "    submission_zip = os.path.join(conf.data_folder,'answer.zip')\n",
    "    o_df.to_csv(submission_csv,index=False)\n",
    "    os.system('rm {}'.format(submission_zip))\n",
    "    os.system('zip {} {}'.format(submission_zip, submission_csv))\n",
    "make_submission(positive_probabilities=submit_proba_[:,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
